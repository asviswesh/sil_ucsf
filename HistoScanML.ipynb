{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uOrfTools\n",
    "from Sequence import DnaSequence\n",
    "from Genome import Locus\n",
    "from GenomeFactory import GenomeFactory\n",
    "from CdtFile import CdtFile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle, sample, seed\n",
    "from Sequence import DnaSequence\n",
    "from random import shuffle, sample, seed\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ROC(fname, positives, negatives):\n",
    "    out = csv.writer(open(fname,\"w\"))\n",
    "    out.writerow(positives)\n",
    "    out.writerow(negatives)\n",
    "    del out\n",
    "\n",
    "def load_ROC(fname):\n",
    "    fin = csv.reader(open(fname))\n",
    "    positives = [float(i) for i in next(fin)]\n",
    "    negatives = [float(i) for i in next(fin)]\n",
    "    return positives,negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orfList = load_ROC(\"uOrfTEATG.csv\")\n",
    "\n",
    "positiveOrf = orfList[0]\n",
    "negativeOrf = orfList[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Sarah's data + genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cdt, ti, ni1, ni2, ni3, ni4) = uOrfTools.load_TE_data()\n",
    "(f, genome1, genome2, genome3, genome4) = uOrfTools.grabGenomes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter for genes conserved across all 4 strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cdt3s, cdt3s_10) = uOrfTools.prepareCdt(cdt, ti, ni1, ni2, ni3, ni4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Danny's data and filter for genes with RNA structure predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "raw1 = list(reader(open(\"utr5_HcG217B_utrdata.csv\")))\n",
    "data1 = raw1[1:]\n",
    "name2constrained = dict((i[0], float(i[2])) for i in data1 if(i[2] != \"-2e+20\"))\n",
    "cdt3s = CdtFile.fromPrototype(\n",
    "    cdt3s, probes = [row for row in cdt3s if (row.extra[ni3] in name2constrained)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting top five lengths of orfs for the 3819 genes used (some had less than five)\n",
    "#using the G217B strain for fungi\n",
    "def topNOrfs(cdt,ni,genome):\n",
    "    \"\"\"Traverses through the Histoplasma genome, finds up to 5 longest uORFs, \n",
    "       returns their lengths and locations as two lists\"\"\"\n",
    "    lengths = []\n",
    "    locations = []\n",
    "    no_ORF = 0\n",
    "    for histogene in cdt:\n",
    "        gene = genome.getGene(histogene.extra[ni])\n",
    "        try:\n",
    "            (utr_five, utr_three) = gene.utr_seqs()\n",
    "\n",
    "        except:\n",
    "            # For genes with no ORF, 5' UTR is undefined\n",
    "            # Report no uORF in this case\n",
    "            lengths.append(0)\n",
    "            no_ORF += 1\n",
    "            continue\n",
    "        x = []\n",
    "        for(start, stop) in utr_five.disjoint_cds(startcodons = [\"ATG\"]):\n",
    "            value = stop - start + 1\n",
    "            x.append((value,start))\n",
    "            \n",
    "        x.sort(reverse = True)\n",
    "        x_lengths = [0]*5\n",
    "        x_locations = [0]*5\n",
    "        for (n,i) in enumerate(x[:5]):\n",
    "            x_lengths[n] = i[0]\n",
    "            x_locations[n] = i[1]\n",
    "        lengths.append(x_lengths)\n",
    "        locations.append(x_locations)\n",
    "\n",
    "        \n",
    "    print \"No ORF for %d genes\" % no_ORF\n",
    "    return lengths,locations\n",
    "topLengths,locLengths = topNOrfs(cdt3s, ni3, genome3)\n",
    "len(topLengths),len(locLengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topLengths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grabbing energy of constrained RNA structure in a list\n",
    "\n",
    "Gfold = []\n",
    "def getGFold(cdt):\n",
    "    Gfold = []\n",
    "    for row in cdt:\n",
    "        gene_name = row.extra[ni3]\n",
    "        if type(name2constrained) == \"int\":\n",
    "            Gfold.append(name2constrained[gene_name] + 0.0)\n",
    "        else:\n",
    "            Gfold.append(float(name2constrained[gene_name]))\n",
    "    return Gfold\n",
    "Gfold = getGFold(cdt3s)\n",
    "len(Gfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get UTR values\n",
    "def findUtrLength(cdt,ni,genome, shuffling_control = False):\n",
    "    \"\"\"Traverses through the Histoplasma genome, finds uORFs, returns max length of uORFs\"\"\"\n",
    "    lengths = []\n",
    "    no_ORF = 0\n",
    "    for histogene in cdt:\n",
    "        gene = genome.getGene(histogene.extra[ni])\n",
    "        try:\n",
    "            (utr_five, utr_three) = gene.utr_seqs()\n",
    "        except:\n",
    "            # For genes with no ORF, 5' UTR is undefined\n",
    "            # Report no uORF in this case\n",
    "            lengths.append(0)\n",
    "            no_ORF += 1\n",
    "            continue\n",
    "        lengths.append(len(utr_five))    \n",
    "    print \"No ORF for %d genes\" % no_ORF\n",
    "    return lengths\n",
    "utrLengths = findUtrLength(cdt3s, ni3, genome3, shuffling_control = True)\n",
    "len(utrLengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probList():\n",
    "    p_start = 1/64.\n",
    "    p_cds = 61/64.0\n",
    "\n",
    "    L = np.array(utrLengths, dtype = \"float\")\n",
    "    n = np.array([i [0] for i in topLengths], dtype = \"float\")/3.\n",
    "    #p_predictions = np.where(n != 0.0, -np.log(1-np.exp((L-3*n)*np.log(1-p_start*np.power(p_cds,n-2)))),0.0)\n",
    "    p_predictions = -np.log(1-np.exp((L-3*n)*np.log(1-p_start*np.power(p_cds,n-2))))\n",
    "    print type(p_predictions)\n",
    "    return p_predictions\n",
    "    #print len(p_predictions),p_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utrList = probList()\n",
    "print utrList\n",
    "print len(utrList)\n",
    "newUtrLengths = []\n",
    "nullValue = 0\n",
    "for i in range(len(utrLengths)):\n",
    "    newUtrLengths.append(utrList[i])\n",
    "    if np.isinf(utrList[i]):\n",
    "        nullValue += 1\n",
    "        utrList[i] = 0.0\n",
    "#print newUtrLengths  \n",
    "print min(utrList)\n",
    "print max(utrList)\n",
    "print None in newUtrLengths\n",
    "print nullValue\n",
    "#uniqueValues, occurCount = np.unique(newUtrLengths, return_counts=True)\n",
    "#print(uniqueValues, occurCount)\n",
    "#np.nan_to_num(newUtrLengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TeValues = []\n",
    "for row in cdt3s:\n",
    "    TeValues.append(row[ti])\n",
    "len(TeValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_names = []\n",
    "for row in cdt3s:\n",
    "    gene_names.append(row.extra[ni3])\n",
    "len(gene_names),gene_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_ATG = []\n",
    "negatives_ATG = []\n",
    "X = [i[0] for i in topLengths]\n",
    "for i in range(len(TeValues)):\n",
    "    if (TeValues[i]) < -2:\n",
    "        positives_ATG.append(X[i])\n",
    "    elif (TeValues[i] <= 2 ):\n",
    "        negatives_ATG.append(X[i])\n",
    "        \n",
    "fig = plt.figure()      \n",
    "probPlot = uOrfTools.rocPlot(fig, positives_ATG, negatives_ATG, color = \"purple\")\n",
    "fig.savefig(\"uOrfTEATG.png\")\n",
    "save_ROC(\"uOrfTEATG.csv\",positives_ATG,negatives_ATG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write features to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCsvFeatures5(orfLengths, rnaStructureLengths, utrLengths, locations, TEValues, gene_names):\n",
    "    for i in (orfLengths, rnaStructureLengths, utrLengths, TEValues, gene_names):\n",
    "        assert(len(i) == len(locations))\n",
    "    with open('histoDataRegression5.csv', 'wb') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter = ',', quotechar = '|', quoting = csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow([\n",
    "                'Top five upstream Orf 1', 'Top five upstream Orf 2',\n",
    "                             'Top five upstream Orf 3', 'Top five upstream Orf 4', \n",
    "                             'Top five upstream Orf 5', \n",
    "                             'Energy of Constrained RNA structure', \n",
    "                             'Location of uORF relative to 5 prime UTR 1', 'Location of uORF relative to 5 prime UTR 2',\n",
    "                             'Location of uORF relative to 5 prime UTR 3', 'Location of uORF relative to 5 prime UTR 4',\n",
    "                             'Location of uORF relative to 5 prime UTR 5', \n",
    "                             'Length of five prime UTR', 'Translation Efficiency Value'])\n",
    "        for i in range(len(locations)):\n",
    "            filewriter.writerow([gene_names[i],orfLengths[i][0], orfLengths[i][1],\n",
    "                                 orfLengths[i][2], orfLengths[i][3],\n",
    "                                 orfLengths[i][4], \n",
    "                                 rnaStructureLengths[i], \n",
    "                                 locations[i][0], locations[i][1],\n",
    "                                 locations[i][2], locations[i][3],\n",
    "                                 locations[i][4],  \n",
    "                                 utrLengths[i], TEValues[i]])\n",
    "\n",
    "testFile = makeCsvFeatures5(topLengths, Gfold, newUtrLengths, locLengths, TeValues, gene_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload features in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "filename = 'histoDataRegression5.csv'\n",
    "dfFrameList = []\n",
    "for chunkTemp in pd.read_csv(filename, chunksize=20):\n",
    "    dfFrameList.append(chunkTemp)\n",
    "dataFrame = pd.concat(dfFrameList)\n",
    "dataFrame.replace(np.Inf, np.nan, inplace= True)\n",
    "dataFrame.replace(-np.Inf, np.nan, inplace= True)\n",
    "dataFrame.fillna(0.0, inplace= True)\n",
    "dataFrame.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the lengths of the data\n",
    "len(utrLengths),len(topLengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the probabilistic model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(1 for i in p_predictions if(i>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positives_prob = []\n",
    "# negatives_prob = []\n",
    "# for i in range(len(p_predictions)):\n",
    "#     if (TeValues[i]) < -2:\n",
    "#         positives_prob.append(p_predictions[i]*100)\n",
    "#     elif (TeValues[i] <= 2 ):\n",
    "#         negatives_prob.append(p_predictions[i]*100)\n",
    "        \n",
    "# #print(len(set(positives)), len(set(negatives)))\n",
    "# fig = plt.figure()      \n",
    "# probPlot = uOrfTools.rocPlot(fig, positives_prob, negatives_prob, color = \"purple\", tmin = -100, tmax = 1000)\n",
    "# #fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_ROC(\"ROC_probabilistic.csv\",positives_prob,negatives_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding secondary attributes\n",
    "dataFrame['Sum of Top 5 Orf'] = dataFrame['Top five upstream Orf 1'] +\\\n",
    "dataFrame['Top five upstream Orf 2'] +\\\n",
    "dataFrame['Top five upstream Orf 3'] +\\\n",
    "dataFrame['Top five upstream Orf 4'] +\\\n",
    "dataFrame['Top five upstream Orf 5']\n",
    "\n",
    "\n",
    "dataFrame['ratio of sum length of 5 Orf to 5 prime UTR length'] = \\\n",
    "dataFrame['Sum of Top 5 Orf']/dataFrame['Length of five prime UTR']\n",
    "dataFrame.replace(np.Inf, np.nan, inplace= True)\n",
    "dataFrame.fillna(0, inplace = True)\n",
    "\n",
    "\n",
    "#Reaarange colums so that Translational Efficiency is the last column\n",
    "cols = dataFrame.columns.tolist()\n",
    "cols = cols[-2:] + cols[:-2]\n",
    "dataFrame = dataFrame[cols]\n",
    "cols = dataFrame.columns.tolist()\n",
    "print cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at correlation\n",
    "corr_matrix = dataFrame.corr()\n",
    "print corr_matrix['Translation Efficiency Value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing to see if I can access a value in the data frame\n",
    "dataFrame.loc[\"ucsf_hc.01_1.G217B.08341\",['Translation Efficiency Value']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "#taking all features\n",
    "#Decision Tree Regression\n",
    "from sklearn.tree import DecisionTreeRegressor, _tree\n",
    "\n",
    "#Get all the features i.e. all the columns names\n",
    "finalCols = dataFrame.columns\n",
    "#Take all columns names of features except last column name which has label\n",
    "finalFeatures = finalCols[:-1]\n",
    "# Get values of all features\n",
    "x = dataFrame.loc[:, finalFeatures].values\n",
    "# Get all values of the label\n",
    "y = dataFrame.loc[:,['Translation Efficiency Value']].values\n",
    "\n",
    "\n",
    "#Stanardize the data because the scales of the features have a large range\n",
    "scaler = StandardScaler().fit(x)\n",
    "x_std_data = scaler.transform(x)\n",
    "#y_std_data = StandardScaler().fit_transform(y)\n",
    "y_std_data = y\n",
    "\n",
    "#Split data into test and train\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_std_data, y_std_data, test_size = 0.2)\n",
    "\n",
    "#Decision tree regressor\n",
    "tree_model = DecisionTreeRegressor(max_depth=2)\n",
    "tree_model.fit(x_train, y_train)\n",
    "\n",
    "#Just look at the training metrics by using the model to predicting on itself\n",
    "y_train_predictions = tree_model.predict(x_train)\n",
    "print(\"Mean absolute error-training data: %0.2f\" % mean_absolute_error(y_train, y_train_predictions))\n",
    "print(\"Mean squared error-training data: %0.2f\" % mean_squared_error(y_train, y_train_predictions))\n",
    "print(\"Root mean squared error-training data: %0.2f\" % np.sqrt(mean_squared_error(y_train, y_train_predictions)))\n",
    "print(\"R2 score-training data: %0.2f\" % r2_score(y_train, y_train_predictions, multioutput = 'variance_weighted'))\n",
    "print(\"Explained variance score-training data : %0.2f\" % explained_variance_score(y_train, y_train_predictions))\n",
    "\n",
    "#Do the predictions on test data and check the metrics\n",
    "y_predictions = tree_model.predict(x_test)\n",
    "print(\"Mean absolute error-test data : %0.2f\" % mean_absolute_error(y_test, y_predictions))\n",
    "print(\"Mean squared error-test data : %0.2f\" % mean_squared_error(y_test, y_predictions))\n",
    "print(\"Root mean squared error-test data : %0.2f\" % np.sqrt(mean_squared_error(y_test, y_predictions)))\n",
    "print(\"R2 score-test data : %0.2f\" % r2_score(y_test, y_predictions, multioutput = 'variance_weighted'))\n",
    "print(\"Explained variance score-test data : %0.2f\" % explained_variance_score(y_test, y_predictions))\n",
    "\n",
    "#Do Predictions on the entire data and check the metrics\n",
    "tree_model.fit(x_std_data, y_std_data)\n",
    "y_whole_predictions = tree_model.predict(x_std_data)\n",
    "print(\"Mean absolute error-whole data: %0.2f\" % mean_absolute_error(y_std_data, y_whole_predictions))\n",
    "print(\"Mean squared error-whole data: %0.2f\" % mean_squared_error(y_std_data, y_whole_predictions))\n",
    "print(\"Root mean squared error-whole data: %0.2f\" % np.sqrt(mean_squared_error(y_std_data, y_whole_predictions)))\n",
    "print(\"R2 score-training data: %0.2f\" % r2_score(y_std_data, y_whole_predictions, multioutput = 'variance_weighted'))\n",
    "print(\"Explained variance score-training data : %0.2f\" % explained_variance_score(y_std_data, y_whole_predictions))\n",
    "\n",
    "\n",
    "\n",
    "#getting the nodes\n",
    "#TODO - modify the code\n",
    "#https://stackoverflow.com/questions/20224526/how-to-extract-the-decision-rules-from-scikit-learn-decision-tree\n",
    "#Trying another Stack Overflow Code\n",
    "def get_code(tree, feature_names):\n",
    "        left      = tree.tree_.children_left\n",
    "        right     = tree.tree_.children_right\n",
    "        threshold = tree.tree_.threshold\n",
    "        features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "        value = tree.tree_.value\n",
    "\n",
    "        def recurse(left, right, threshold, features, node):\n",
    "                if (threshold[node] != -2):\n",
    "                        print \"if ( \" + features[node] + \" <= \" + str(threshold[node]) + \" ) {\"\n",
    "                        if left[node] != -1:\n",
    "                                recurse (left, right, threshold, features,left[node])\n",
    "                        print \"} else {\"\n",
    "                        if right[node] != -1:\n",
    "                                recurse (left, right, threshold, features,right[node])\n",
    "                        print \"}\"\n",
    "                else:\n",
    "                        print \"return \" + str(value[node])\n",
    "\n",
    "        recurse(left, right, threshold, features, 0)\n",
    "\n",
    "\n",
    "get_code(tree_model, finalCols)\n",
    "\n",
    "\n",
    "print(tree_model.get_params())\n",
    "#print(tree_model.decision_path(x_train))\n",
    "\n",
    "#Compare the predicted values with real values\n",
    "df = pd.DataFrame({'Actual':y_test.flatten(),'Predicted':y_predictions.flatten()})\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "from matplotlib import pyplot\n",
    "y2 = y\n",
    "#y2 = 1/np.log(np.sqrt(y2+10))\n",
    "pyplot.hist(y2)\n",
    "fig\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "qqplot(y2, line='s')\n",
    "fig\n",
    "dfy2 = pd.DataFrame(y2)\n",
    "dfy2.describe()\n",
    "# from scipy.stats import normaltest\n",
    "# stat, p = normaltest(y)\n",
    "# print ('statistics=%.3f, p=%.3f' %(stat,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.flatten()\n",
    "y_train = y_train.flatten()\n",
    "y_predictions = y_predictions.flatten()\n",
    "y_train_predictions = y_train_predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "#ROC plot for Decision Tree Regressor \n",
    "fig = plt.figure()\n",
    "positivesTree = []\n",
    "negativesTree = []\n",
    "\n",
    "for i in range(len(y_predictions)):\n",
    "    if (y_test[i]) < -2:\n",
    "        positivesTree.append(-y_predictions[i]*100)\n",
    "    elif (y_test[i] <= 2):\n",
    "        negativesTree.append(-y_predictions[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "        \n",
    "treePlot = uOrfTools.rocPlot(fig, positivesTree, negativesTree, color = \"green\", tmin = -100, tmax = 700)\n",
    "save_ROC(\"ROC_dTree.csv\",positivesTree,negativesTree)\n",
    "fig.savefig(\"rocTreeTest.png\")\n",
    "#fig2 = plt.figure()\n",
    "#b = plt.boxplot((positives,negatives))\n",
    "\n",
    "#fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "#ROC plot for Decision Tree Regressor \n",
    "fig = plt.figure()\n",
    "positivesTree = []\n",
    "negativesTree = []\n",
    "\n",
    "for i in range(len(y_train_predictions)):\n",
    "    if (y_train[i]) < -2:\n",
    "        positivesTree.append(-y_train_predictions[i]*100)\n",
    "    elif (y_train[i] <= 2):\n",
    "        negativesTree.append(-y_train_predictions[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "        \n",
    "treePlot = uOrfTools.rocPlot(fig, positivesTree, negativesTree, color = \"purple\", tmin = -100, tmax = 700)\n",
    "save_ROC(\"ROC_dTree.train.csv\",positivesTree,negativesTree)\n",
    "fig.savefig(\"rocTreeTrain.png\")\n",
    "#fig2 = plt.figure()\n",
    "#b = plt.boxplot((positives,negatives))\n",
    "\n",
    "#fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot for full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "#ROC plot for Decision Tree Regressor \n",
    "fig = plt.figure()\n",
    "positivesTree = []\n",
    "negativesTree = []\n",
    "\n",
    "X = np.hstack((y_train,y_test))\n",
    "Y = np.hstack((y_train_predictions,y_predictions))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if (X[i]) < -2:\n",
    "        positivesTree.append(-Y[i]*100)\n",
    "    elif (X[i] <= 2):\n",
    "        negativesTree.append(-Y[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "        \n",
    "treePlot = uOrfTools.rocPlot(fig, positivesTree, negativesTree, color = \"purple\", tmin = -100, tmax = 700)\n",
    "uOrfTools.rocPlot(fig, positiveOrf, negativeOrf, color = \"red\", tmin = -100, tmax = 700)\n",
    "\n",
    "fig.savefig(\"rocTreeAll.png\")\n",
    "save_ROC(\"ROC_dTree.full.csv\",positivesTree,negativesTree)\n",
    "#fig2 = plt.figure()\n",
    "#b = plt.boxplot((positives,negatives))\n",
    "\n",
    "#fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdrPlot(fig,positives, negatives, color = \"gray\",\n",
    "            tmin = 0, tmax = 500, tstep = 1, marker = \"None\"):\n",
    "    totalPositives = float(len(positives))\n",
    "    totalNegatives = float(len(negatives))\n",
    "    fdr = []\n",
    "    thresh = []\n",
    "    for threshold in range(tmin,tmax+1, tstep):\n",
    "        #TODO set counter variables\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        for i in range(len(positives)):\n",
    "            if positives[i] >= threshold:\n",
    "                tp = tp + 1\n",
    "                \n",
    "        for i in range(len(negatives)):\n",
    "            if negatives[i] >= threshold:\n",
    "                fp = fp + 1\n",
    "\n",
    "        fdRate = fp/totalNegatives\n",
    "        fdr.append(fdRate)\n",
    "        thresh.append(threshold)\n",
    "        \n",
    "    \n",
    "    roc_plot = plt.plot(fdr, thresh, color=color,marker=marker)\n",
    "    plt.xlabel(\"fdr\")\n",
    "    plt.ylabel(\"threshold\")\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig_fp = fdrPlot(fig, positivesTree, negativesTree, color = \"purple\", tmin = -100, tmax = 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the sklearn documentation: https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
    "\n",
    "from Colors import ColorServer\n",
    "cs = ColorServer()\n",
    "feature_colors = [cs[i] for i in finalFeatures]\n",
    "\n",
    "class MyTree:\n",
    "    def __init__(self, t_children_left,t_children_right,t_threshold,t_feature,t_value, feature_names, \n",
    "                 feature_colors, p_thresh=1):\n",
    "        self.children_left = t_children_left\n",
    "        self.children_right = t_children_right\n",
    "        self.threshold = t_threshold\n",
    "        self.feature = t_feature\n",
    "        self.value = t_value\n",
    "        self.feature_names = feature_names\n",
    "        self.feature_colors = feature_colors\n",
    "        self.p_thresh = p_thresh\n",
    "        \n",
    "    @classmethod\n",
    "    def fromDT(cls, tree, feature_names, feature_colors, p_thresh = 1):\n",
    "        t = tree.tree_\n",
    "        return cls(t.children_left,t.children_right,t.threshold,t.feature,[i[0][0] for i in t.value], feature_names, \n",
    "                 feature_colors, p_thresh)\n",
    "        \n",
    "    def node_label(self, n):\n",
    "        if(self.children_left[n] == self.children_right[n]):\n",
    "            # Leaf node\n",
    "            return \"%d: %3.2f\" % (n, self.value[n])\n",
    "        \n",
    "        # Internal node\n",
    "        return \"%d: %s <= %3.2f\" % (n, self.feature_names[self.feature[n]], self.threshold[n])\n",
    "        \n",
    "    def to_graphviz(self):\n",
    "        node_properties = \"\"\n",
    "        s = 'digraph G{\\nrankdir=\"LR\";\\n'\n",
    "        for (n,(left,right)) in enumerate(zip(self.children_left, self.children_right)):\n",
    "            if(left == right):\n",
    "                # Leaf node\n",
    "                if(self.value[n] < self.p_thresh):\n",
    "                    color = \"red\"\n",
    "                else:\n",
    "                    color = \"green\"\n",
    "                node_properties += '\"%s\" [style=filled fillcolor=%s];\\n' % (self.node_label(n),color)\n",
    "            else:\n",
    "                # Internal node\n",
    "                parent = self.node_label(n)\n",
    "                s += '  \"%s\" -> \"%s\" [label=\"yes\"];\\n' % (parent, self.node_label(left))\n",
    "                s += '  \"%s\" -> \"%s\" [label=\"no\"];\\n' % (parent, self.node_label(right))\n",
    "                node_properties += '\"%s\" [style=filled fillcolor=\"%s\"];\\n' % (parent, \n",
    "                                                                              self.feature_colors[self.feature[n]])\n",
    "        s += node_properties+\"}\\n\"\n",
    "        return s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = MyTree.fromDT(tree_model, finalFeatures, feature_colors, p_thresh=-2.32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"DT_2_32.dot\",\"w\").write(tree.to_graphviz())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -Tpng DT_2_32.dot > DT_2_32.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"DT_2_32.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_values = ('Top five upstream Orf 1', 'Top five upstream Orf 2',\n",
    "#                              'Top five upstream Orf 3', 'Top five upstream Orf 4', \n",
    "#                              'Top five upstream Orf 5', \n",
    "#                              'Energy of Constrained RNA structure', \n",
    "#                              'Location of uORF relative to 5 prime UTR 1', 'Location of uORF relative to 5 prime UTR 2',\n",
    "#                              'Location of uORF relative to 5 prime UTR 3', 'Location of uORF relative to 5 prime UTR 4',\n",
    "#                              'Location of uORF relative to 5 prime UTR 5', \n",
    "#                              'Length of five prime UTR')\n",
    "# y_pos = np.arange(len(x_values))\n",
    "# print y_pos\n",
    "# print type(y_pos) \n",
    "# #y_values = model.coef_[0] * 100000\n",
    "# #print y_values\n",
    "# #print tree_model.coef_ * 100000\n",
    "# fig = plt.figure()\n",
    "# plt.bar(range(tree_model.coef_.shape[1]), tree_model.coef[0])\n",
    "# plt.xlabel('Features of the Algorithm')\n",
    "# plt.ylabel('Value of the Coefficients')\n",
    "# plt.savefig('treeBar.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression - Proper code starts here\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Get all the features i.e. all the columns names\n",
    "finalCols = dataFrame.columns\n",
    "#Take all columns names of features except last column name which has label\n",
    "finalFeatures = finalCols[:-1]\n",
    "# Get values of all features\n",
    "x = dataFrame.loc[:, finalFeatures].values\n",
    "# Get all values of the label\n",
    "y = dataFrame.loc[:,['Translation Efficiency Value']].values\n",
    "\n",
    "\n",
    "#Standardize the data because the scales of the features have a large range\n",
    "scaler = StandardScaler().fit(x)\n",
    "x_std_data = scaler.transform(x)\n",
    "#y_std_data = StandardScaler().fit_transform(y)\n",
    "y_std_data = y\n",
    "\n",
    "#Split data into test and train\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_std_data, y_std_data, test_size = 0.2)\n",
    "\n",
    "#Try polynomial regression, so have to first make the features polynomial\n",
    "#reverted to degree=1 as higher degrees gives worse metrics\n",
    "poly_reg = PolynomialFeatures(degree = 1)\n",
    "x_poly = poly_reg.fit_transform(x_train)\n",
    "x_test_poly = poly_reg.fit_transform(x_test)\n",
    "\n",
    "#Linear Regression\n",
    "model = LinearRegression(normalize = True, fit_intercept=True)\n",
    "model.fit(x_poly, y_train)\n",
    "print \"model coefficients for train data\"\n",
    "print(model.coef_)\n",
    "\n",
    "#Just look at the training metrics by using the model to predicting on itself\n",
    "y_train_predictions = model.predict(x_poly)\n",
    "print(\"Mean absolute error-training data: %0.2f\" % mean_absolute_error(y_train, y_train_predictions))\n",
    "print(\"Mean squared error-training data: %0.2f\" % mean_squared_error(y_train, y_train_predictions))\n",
    "print(\"Root mean squared error-training data: %0.2f\" % np.sqrt(mean_squared_error(y_train, y_train_predictions)))\n",
    "print(\"R2 score-training data: %0.2f\" % r2_score(y_train, y_train_predictions, multioutput = 'variance_weighted'))\n",
    "print(\"Explained variance score-training data : %0.2f\" % explained_variance_score(y_train, y_train_predictions))\n",
    "\n",
    "\n",
    "#Predict on test data\n",
    "y_predictions = model.predict(x_test_poly)\n",
    "print(\"Mean absolute error-test data: %0.2f\" % mean_absolute_error(y_test, y_predictions))\n",
    "print(\"Mean squared error-test data : %0.2f\" % mean_squared_error(y_test, y_predictions))\n",
    "print(\"Root mean squared error-test data : %0.2f\" % np.sqrt(mean_squared_error(y_test, y_predictions)))\n",
    "print(\"R2 score-test data : %0.2f\" % r2_score(y_test, y_predictions, multioutput = 'variance_weighted'))\n",
    "print(\"Explained variance score-test data : %0.2f\" % explained_variance_score(y_test, y_predictions))\n",
    "\n",
    "#Do Predictions on the entire data and check the metrics\n",
    "model.fit(x_std_data, y_std_data)\n",
    "y_whole_predictions = model.predict(x_std_data)\n",
    "print \"model coefficients for whole data\"\n",
    "print(model.coef_)\n",
    "print(\"Mean absolute error-whole data: %0.2f\" % mean_absolute_error(y_std_data, y_whole_predictions))\n",
    "print(\"Mean squared error-whole data: %0.2f\" % mean_squared_error(y_std_data, y_whole_predictions))\n",
    "print(\"Root mean squared error-whole data: %0.2f\" % np.sqrt(mean_squared_error(y_std_data, y_whole_predictions)))\n",
    "print(\"R2 score-training data: %0.2f\" % r2_score(y_std_data, y_whole_predictions, multioutput = 'variance_weighted'))\n",
    "print(\"Explained variance score-training data : %0.2f\" % explained_variance_score(y_std_data, y_whole_predictions))\n",
    "\n",
    "df = pd.DataFrame({'Actual':y_test.flatten(),'Predicted':y_predictions.flatten()})\n",
    "print df.head(20)\n",
    "#check to what extent weights are most prominent\n",
    "#throw out the factors it is not making use on\n",
    "#do we do as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.flatten()\n",
    "y_train = y_train.flatten()\n",
    "y_predictions = y_predictions.flatten()\n",
    "y_train_predictions = y_train_predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression roc plot\n",
    "fig = plt.figure()\n",
    "positives = []\n",
    "negatives = []\n",
    "\n",
    "for i in range(len(y_predictions)):\n",
    "    if (y_test[i] < -2):\n",
    "        #Question for Dr. Voorhies: why should I take -ve of y_predictions\n",
    "        positives.append(-y_predictions[i]*100)\n",
    "    elif (y_test[i] <=2):\n",
    "        negatives.append(-y_predictions[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "print len(positives)\n",
    "print len(negatives)\n",
    "        \n",
    "linearPlot = uOrfTools.rocPlot(fig, positives, negatives, color = \"orange\", tmin = -100, tmax = 700)\n",
    "fig.savefig(\"rocLinearTest.png\")\n",
    "save_ROC(\"ROC_linear.csv\",positives,negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression roc plot\n",
    "fig = plt.figure()\n",
    "positives = []\n",
    "negatives = []\n",
    "\n",
    "for i in range(len(y_train_predictions)):\n",
    "    if (y_train[i] < -2):\n",
    "        #Question for Dr. Voorhies: why should I take -ve of y_predictions\n",
    "        positives.append(-y_train_predictions[i]*100)\n",
    "    elif (y_train[i] <=2):\n",
    "        negatives.append(-y_train_predictions[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "print len(positives)\n",
    "print len(negatives)\n",
    "        \n",
    "linearPlot = uOrfTools.rocPlot(fig, positives, negatives, color = \"green\", tmin = -100, tmax = 700)\n",
    "fig.savefig(\"rocLinearTraining.png\")\n",
    "save_ROC(\"ROC_linear.train.csv\",positives,negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = ('Orf 1', 'Orf 2',\n",
    "                             'Orf 3', 'Orf 4', \n",
    "                             'Orf 5', \n",
    "                             'RNA', \n",
    "                             'Loc 1', 'Loc 2',\n",
    "                             'Loc 3', 'Loc 4',\n",
    "                             'Loc 5', \n",
    "                             'UTR')\n",
    "# y_pos = np.arange(len(x_values))\n",
    "# print y_pos\n",
    "# print type(y_pos) \n",
    "# y_values = model.coef_[0]\n",
    "# print y_values\n",
    "# print model.coef_ * 100000\n",
    "fig = plt.figure()\n",
    "plt.bar(range(model.coef_.shape[1]), model.coef_[0])\n",
    "plt.xticks(range(model.coef_.shape[1]), x_values)\n",
    "#plt.xticks(y_pos, x_values)\n",
    "plt.xlabel('Features of the Algorithm')\n",
    "plt.ylabel('Value of the Coefficients')\n",
    "plt.savefig('linearBar.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot for all genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression roc plot\n",
    "fig = plt.figure()\n",
    "positives = []\n",
    "negatives = []\n",
    "X = np.hstack((y_train,y_test))\n",
    "Y = np.hstack((y_train_predictions,y_predictions))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if (X[i] < -2):\n",
    "        #Question for Dr. Voorhies: why should I take -ve of y_predictions\n",
    "        positives.append(-Y[i]*100)\n",
    "    elif (X[i] <=2):\n",
    "        negatives.append(-Y[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "#print len(positives)\n",
    "#print len(negatives)\n",
    "        \n",
    "forestPlot = uOrfTools.rocPlot(fig, positives, negatives, color = \"green\", tmin = -100, tmax = 700)\n",
    "uOrfTools.rocPlot(fig, positiveOrf, negativeOrf, color = \"red\", tmin = -100, tmax = 700)\n",
    "fig.savefig(\"rocLinearAllATG.png\")\n",
    "save_ROC(\"ROC_linear.full.csv\",positives,negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso regression \n",
    "#REDO WITH CORRECT ALPHA VALUE\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score, r2_score, explained_variance_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Get all the features i.e. all the columns names\n",
    "finalCols = dataFrame.columns\n",
    "#Take all columns names of features except last column name which has label\n",
    "finalFeatures = finalCols[:-1]\n",
    "# Get values of all features\n",
    "x = dataFrame.loc[:, finalFeatures].values\n",
    "# Get all values of the label\n",
    "y = dataFrame.loc[:,['Translation Efficiency Value']].values\n",
    "\n",
    "print finalFeatures\n",
    "\n",
    "#Standardize the data because the scales of the features have a large range\n",
    "scaler = StandardScaler().fit(x)\n",
    "x_std_data = scaler.transform(x)\n",
    "#y_std_data = StandardScaler().fit_transform(y)\n",
    "y_std_data = y\n",
    "\n",
    "#Split data into test and train\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_std_data, y_std_data, test_size = 0.2)\n",
    "\n",
    "#check to what extent weights are most prominent\n",
    "#throw out the factors it is not making use on\n",
    "#do we do as well\n",
    "#Try LassoCV\n",
    "#Lasso regression\n",
    "#Lasso is a type of Linear Regression\n",
    "#uses shrinkage, where data values are shrunk towards a central data point, ex. mean\n",
    "#well-suite for models with multicollinearity\n",
    "#performs L1 regularization, adds penalty equal to abs value of coefs\n",
    "#which means some coefs can be zeroes\n",
    "from sklearn.linear_model import LassoCV\n",
    "lassoCV_model = LassoCV(cv=5, random_state=0, max_iter=10000)\n",
    "lassoCV_model.fit(x_train, y_train)\n",
    "print('Train data coefficients', lassoCV_model.coef_)\n",
    "print('Train data alpha', lassoCV_model.alpha_)\n",
    "print lassoCV_model\n",
    "\n",
    "#Just look at the training metrics by using the model to predicting on itself\n",
    "y_train_predictions = lassoCV_model.predict(x_train)\n",
    "print(\"Mean absolute error-training data: %0.2f\" % mean_absolute_error(y_train, y_train_predictions))\n",
    "print(\"Mean squared error-training data: %0.2f\" % mean_squared_error(y_train, y_train_predictions))\n",
    "print(\"Root mean squared error-training data: %0.2f\" % np.sqrt(mean_squared_error(y_train, y_train_predictions)))\n",
    "print(\"R2 score-training data: %0.2f\" % r2_score(y_train, y_train_predictions, multioutput = 'variance_weighted'))\n",
    "print(\"Explained variance score-training data : %0.2f\" % explained_variance_score(y_train, y_train_predictions))\n",
    "#print('Train coefficients:', lassoCV_model.coef_)\n",
    "\n",
    "#Predict on test data\n",
    "y_predictions = lassoCV_model.predict(x_test)\n",
    "print(\"Mean absolute error-test data: %0.2f\" % mean_absolute_error(y_test, y_predictions))\n",
    "print(\"Mean squared error-test data : %0.2f\" % mean_squared_error(y_test, y_predictions))\n",
    "print(\"Root mean squared error-test data : %0.2f\" % np.sqrt(mean_squared_error(y_test, y_predictions)))\n",
    "print(\"R2 score-test data : %0.2f\" % r2_score(y_test, y_predictions, multioutput = 'variance_weighted'))\n",
    "print(\"Explained variance score-test data : %0.2f\" % explained_variance_score(y_test, y_predictions))\n",
    "#print(lassoCV_model.coef_)\n",
    "\n",
    "\n",
    "#Do Predictions on the entire data and check the metrics\n",
    "lassoCV_model.fit(x_std_data, y_std_data)\n",
    "print('All data coefficients', lassoCV_model.coef_)\n",
    "print('All data alpha', lassoCV_model.alpha_)\n",
    "print lassoCV_model\n",
    "y_whole_predictions = lassoCV_model.predict(x_std_data)\n",
    "print(\"Mean absolute error-whole data: %0.2f\" % mean_absolute_error(y_std_data, y_whole_predictions))\n",
    "print(\"Mean squared error-whole data: %0.2f\" % mean_squared_error(y_std_data, y_whole_predictions))\n",
    "print(\"Root mean squared error-whole data: %0.2f\" % np.sqrt(mean_squared_error(y_std_data, y_whole_predictions)))\n",
    "print(\"R2 score-training data: %0.2f\" % r2_score(y_std_data, y_whole_predictions, multioutput = 'variance_weighted'))\n",
    "print(\"Explained variance score-training data : %0.2f\" % explained_variance_score(y_std_data, y_whole_predictions))\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Actual':y_test.flatten(),'Predicted':y_predictions.flatten()})\n",
    "print df.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.flatten()\n",
    "y_train = y_train.flatten()\n",
    "y_predictions = y_predictions.flatten()\n",
    "y_train_predictions = y_train_predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "positivesLasso = []\n",
    "negativesLasso = []\n",
    "\n",
    "for i in range(len(y_predictions)):\n",
    "    if (y_test[i]) < -2:\n",
    "        positivesLasso.append(-y_predictions[i]*100)\n",
    "    elif (y_test[i] <= 2):\n",
    "        negativesLasso.append(-y_predictions[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "        \n",
    "lassoPlot = uOrfTools.rocPlot(fig, positivesLasso, negativesLasso, color = \"purple\", tmin = -100, tmax = 700)\n",
    "fig.savefig(\"rocLassoTest.png\")\n",
    "save_ROC(\"ROC_lasso.csv\",positivesLasso,negativesLasso)\n",
    "#fig2 = plt.figure()\n",
    "#b = plt.boxplot((positives,negatives))\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "positivesLasso = []\n",
    "negativesLasso = []\n",
    "\n",
    "for i in range(len(y_train_predictions)):\n",
    "    if (y_train[i]) < -2:\n",
    "        positivesLasso.append(-y_train_predictions[i]*100)\n",
    "    elif (y_train[i] <= 2):\n",
    "        negativesLasso.append(-y_train_predictions[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "        \n",
    "lassoPlot = uOrfTools.rocPlot(fig, positivesLasso, negativesLasso, color = \"purple\", tmin = -100, tmax = 700)\n",
    "fig\n",
    "\n",
    "fig.savefig(\"rocLassoTrain.png\")\n",
    "save_ROC(\"ROC_lasso.train.csv\",positivesLasso,negativesLasso)\n",
    "#fig2 = plt.figure()\n",
    "#b = plt.boxplot((positives,negatives))\n",
    "\n",
    "#fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot for all genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape, y_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "positivesLasso = []\n",
    "negativesLasso = []\n",
    "\n",
    "X = np.hstack((y_test,y_train))\n",
    "Y = np.hstack((y_predictions,y_train_predictions))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if (X[i]) < -2:\n",
    "        positivesLasso.append(-Y[i]*100)\n",
    "    elif (X[i] <= 2):\n",
    "        negativesLasso.append(-Y[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "        \n",
    "lassoPlot = uOrfTools.rocPlot(fig, positivesLasso, negativesLasso, color = \"purple\", tmin = -100, tmax = 700)\n",
    "uOrfTools.rocPlot(fig, positiveOrf, negativeOrf, color = \"red\", tmin = -100, tmax = 700)\n",
    "fig.savefig(\"rocLassoAllATG.png\")\n",
    "save_ROC(\"ROC_lasso.full.csv\",positivesLasso,negativesLasso)\n",
    "#fig2 = plt.figure()\n",
    "#b = plt.boxplot((positives,negatives))\n",
    "\n",
    "#fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = ('Orf 1', 'Orf 2',\n",
    "                             'Orf 3', 'Orf 4', \n",
    "                             'Orf 5', \n",
    "                             'RNA', \n",
    "                             'Loc 1', 'Loc 2',\n",
    "                             'Loc 3', 'Loc 4',\n",
    "                             'Loc 5', \n",
    "                             'UTR')\n",
    "# y_pos = np.arange(len(x_values))\n",
    "# print y_pos\n",
    "# print type(y_pos) \n",
    "# y_values = model.coef_[0]\n",
    "# print y_values\n",
    "# print model.coef_ * 100000\n",
    "fig = plt.figure()\n",
    "plt.bar(x_values, lassoCV_model.coef_[0])\n",
    "#plt.xticks(range(lassoCV_model.coef_.shape[1]), x_values)\n",
    "#plt.xticks(y_pos, x_values)\n",
    "plt.xlabel('Features of the Algorithm')\n",
    "plt.ylabel('Value of the Coefficients')\n",
    "plt.savefig('lassoBar.png')\n",
    "print lassoCV_model.coef_.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge regression \n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score, r2_score, explained_variance_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Get all the features i.e. all the columns names\n",
    "finalCols = dataFrame.columns\n",
    "#Take all columns names of features except last column name which has label\n",
    "finalFeatures = finalCols[:-1]\n",
    "# Get values of all features\n",
    "x = dataFrame.loc[:, finalFeatures].values\n",
    "# Get all values of the label\n",
    "y = dataFrame.loc[:,['Translation Efficiency Value']].values\n",
    "\n",
    "print finalFeatures\n",
    "\n",
    "#Standardize the data because the scales of the features have a large range\n",
    "scaler = StandardScaler().fit(x)\n",
    "x_std_data = scaler.transform(x)\n",
    "#y_std_data = StandardScaler().fit_transform(y)\n",
    "y_std_data = y\n",
    "\n",
    "#Split data into test and train\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_std_data, y_std_data, test_size = 0.2)\n",
    "\n",
    "#Ridge is a type of Linear Regression\n",
    "#performs L2 regularization\n",
    "from sklearn.linear_model import RidgeCV\n",
    "ridgeCV_model = RidgeCV(alphas = [0.001,0.01,0.1,1.0,10.0],cv=5, normalize = True, scoring = 'neg_mean_squared_error')\n",
    "ridgeCV_model.fit(x_train, y_train)\n",
    "print('Train data coefficients', ridgeCV_model.coef_)\n",
    "print('Train data alpha', ridgeCV_model.alpha_)\n",
    "print ridgeCV_model\n",
    "\n",
    "\n",
    "#Just look at the training metrics by using the model to predicting on itself\n",
    "y_train_predictions = ridgeCV_model.predict(x_train)\n",
    "print(\"Mean absolute error-training data: %0.2f\" % mean_absolute_error(y_train, y_train_predictions))\n",
    "print(\"Mean squared error-training data: %0.2f\" % mean_squared_error(y_train, y_train_predictions))\n",
    "print(\"Root mean squared error-training data: %0.2f\" % np.sqrt(mean_squared_error(y_train, y_train_predictions)))\n",
    "print(\"R2 score-training data: %0.2f\" % r2_score(y_train, y_train_predictions, multioutput = 'variance_weighted'))\n",
    "print(\"Explained variance score-training data : %0.2f\" % explained_variance_score(y_train, y_train_predictions))\n",
    "\n",
    "#Predict on test data\n",
    "y_predictions = ridgeCV_model.predict(x_test)\n",
    "print(\"Mean absolute error-test data: %0.2f\" % mean_absolute_error(y_test, y_predictions))\n",
    "print(\"Mean squared error-test data : %0.2f\" % mean_squared_error(y_test, y_predictions))\n",
    "print(\"Root mean squared error-test data : %0.2f\" % np.sqrt(mean_squared_error(y_test, y_predictions)))\n",
    "print(\"R2 score-test data : %0.2f\" % r2_score(y_test, y_predictions, multioutput = 'variance_weighted'))\n",
    "print(\"Explained variance score-test data : %0.2f\" % explained_variance_score(y_test, y_predictions))\n",
    "\n",
    "\n",
    "#Do Predictions on the entire data and check the metrics\n",
    "ridgeCV_model.fit(x_std_data, y_std_data)\n",
    "print('All data coefficients', ridgeCV_model.coef_)\n",
    "print('All data alpha', ridgeCV_model.alpha_)\n",
    "print ridgeCV_model\n",
    "y_whole_predictions = ridgeCV_model.predict(x_std_data)\n",
    "print(\"Mean absolute error-whole data: %0.2f\" % mean_absolute_error(y_std_data, y_whole_predictions))\n",
    "print(\"Mean squared error-whole data: %0.2f\" % mean_squared_error(y_std_data, y_whole_predictions))\n",
    "print(\"Root mean squared error-whole data: %0.2f\" % np.sqrt(mean_squared_error(y_std_data, y_whole_predictions)))\n",
    "print(\"R2 score-training data: %0.2f\" % r2_score(y_std_data, y_whole_predictions, multioutput = 'variance_weighted'))\n",
    "print(\"Explained variance score-training data : %0.2f\" % explained_variance_score(y_std_data, y_whole_predictions))\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Actual':y_test.flatten(),'Predicted':y_predictions.flatten()})\n",
    "print df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.flatten()\n",
    "y_train = y_train.flatten()\n",
    "y_predictions = y_predictions.flatten()\n",
    "y_train_predictions = y_train_predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "positivesRidge = []\n",
    "negativesRidge = []\n",
    "\n",
    "for i in range(len(y_predictions)):\n",
    "    if (y_test[i]) < -2:\n",
    "        positivesRidge.append(-y_predictions[i]*100)\n",
    "    elif (y_test[i] <= 2):\n",
    "        negativesRidge.append(-y_predictions[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "        \n",
    "ridgePlot = uOrfTools.rocPlot(fig, positivesRidge, negativesRidge, color = \"purple\", tmin = -100, tmax = 700)\n",
    "fig.savefig(\"rocRidgeTest.png\")\n",
    "save_ROC(\"ROC_ridge.csv\",positivesRidge,negativesRidge)\n",
    "#fig2 = plt.figure()\n",
    "#b = plt.boxplot((positives,negatives))\n",
    "\n",
    "#fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "positivesRidge = []\n",
    "negativesRidge = []\n",
    "\n",
    "for i in range(len(y_train_predictions)):\n",
    "    if (y_train[i]) < -2:\n",
    "        positivesRidge.append(-y_train_predictions[i]*100)\n",
    "    elif (y_train[i] <= 2):\n",
    "        negativesRidge.append(-y_train_predictions[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "        \n",
    "ridgePlot = uOrfTools.rocPlot(fig, positivesRidge, negativesRidge, color = \"purple\", tmin = -100, tmax = 700)\n",
    "fig.savefig(\"rocRidgeTrain.png\")\n",
    "save_ROC(\"ROC_ridge.train.csv\",positivesRidge,negativesRidge)\n",
    "#fig2 = plt.figure()\n",
    "#b = plt.boxplot((positives,negatives))\n",
    "\n",
    "#fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot for all genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "positivesRidge = []\n",
    "negativesRidge = []\n",
    "\n",
    "X = np.hstack((y_test,y_train))\n",
    "Y = np.hstack((y_predictions,y_train_predictions))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if (X[i]) < -2:\n",
    "        positivesRidge.append(-Y[i]*100)\n",
    "    elif (X[i] <= 2):\n",
    "        negativesRidge.append(-Y[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "        \n",
    "ridgePlot = uOrfTools.rocPlot(fig, positivesRidge, negativesRidge, color = \"purple\", tmin = -100, tmax = 700)\n",
    "fig.savefig(\"rocRidgeAll.png\")\n",
    "save_ROC(\"ROC_ridge.full.csv\",positivesRidge,negativesRidge)\n",
    "#fig2 = plt.figure()\n",
    "#b = plt.boxplot((positives,negatives))\n",
    "\n",
    "#fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Linear regression \n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score, r2_score, explained_variance_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Get all the features i.e. all the columns names\n",
    "finalCols = dataFrame.columns\n",
    "#Take all columns names of features except last column name which has label\n",
    "finalFeatures = finalCols[:-1]\n",
    "# Get values of all features\n",
    "x = dataFrame.loc[:, finalFeatures].values\n",
    "# Get all values of the label\n",
    "y = dataFrame.loc[:,['Translation Efficiency Value']].values\n",
    "\n",
    "print finalFeatures\n",
    "\n",
    "#Standardize the data because the scales of the features have a large range\n",
    "scaler = StandardScaler().fit(x)\n",
    "x_std_data = scaler.transform(x)\n",
    "#y_std_data = StandardScaler().fit_transform(y)\n",
    "y_std_data = y\n",
    "\n",
    "#Split data into test and train\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_std_data, y_std_data, test_size = 0.2)\n",
    "\n",
    "#SVM linear regressor\n",
    "from sklearn.svm import SVR\n",
    "svm_linear_reg = SVR(kernel = \"linear\", C = 4.0, epsilon = 0.2)\n",
    "svm_linear_reg.fit(x_train, y_train)\n",
    "print('Train data coefficients', svm_linear_reg.coef_)\n",
    "print svm_linear_reg\n",
    "\n",
    "\n",
    "#Just look at the training metrics by using the model to predicting on itself\n",
    "y_train_predictions = svm_linear_reg.predict(x_train)\n",
    "print(\"Mean absolute error-training data: %0.2f\" % mean_absolute_error(y_train, y_train_predictions))\n",
    "print(\"Mean squared error-training data: %0.2f\" % mean_squared_error(y_train, y_train_predictions))\n",
    "print(\"Root mean squared error-training data: %0.2f\" % np.sqrt(mean_squared_error(y_train, y_train_predictions)))\n",
    "print(\"R2 score-training data: %0.2f\" % r2_score(y_train, y_train_predictions, multioutput = 'variance_weighted'))\n",
    "print(\"Explained variance score-training data : %0.2f\" % explained_variance_score(y_train, y_train_predictions))\n",
    "\n",
    "#Predict on test data\n",
    "y_predictions = ridgeCV_model.predict(x_test)\n",
    "print(\"Mean absolute error-test data: %0.2f\" % mean_absolute_error(y_test, y_predictions))\n",
    "print(\"Mean squared error-test data : %0.2f\" % mean_squared_error(y_test, y_predictions))\n",
    "print(\"Root mean squared error-test data : %0.2f\" % np.sqrt(mean_squared_error(y_test, y_predictions)))\n",
    "print(\"R2 score-test data : %0.2f\" % r2_score(y_test, y_predictions, multioutput = 'variance_weighted'))\n",
    "print(\"Explained variance score-test data : %0.2f\" % explained_variance_score(y_test, y_predictions))\n",
    "\n",
    "\n",
    "#Do Predictions on the entire data and check the metrics\n",
    "svm_linear_reg.fit(x_std_data, y_std_data)\n",
    "print('All data coefficients', svm_linear_reg.coef_)\n",
    "print svm_linear_reg\n",
    "y_whole_predictions = svm_linear_reg.predict(x_std_data)\n",
    "print(\"Mean absolute error-whole data: %0.2f\" % mean_absolute_error(y_std_data, y_whole_predictions))\n",
    "print(\"Mean squared error-whole data: %0.2f\" % mean_squared_error(y_std_data, y_whole_predictions))\n",
    "print(\"Root mean squared error-whole data: %0.2f\" % np.sqrt(mean_squared_error(y_std_data, y_whole_predictions)))\n",
    "print(\"R2 score-training data: %0.2f\" % r2_score(y_std_data, y_whole_predictions, multioutput = 'variance_weighted'))\n",
    "print(\"Explained variance score-training data : %0.2f\" % explained_variance_score(y_std_data, y_whole_predictions))\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Actual':y_test.flatten(),'Predicted':y_predictions.flatten()})\n",
    "print df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot for test data - SVM Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.flatten()\n",
    "y_train = y_train.flatten()\n",
    "y_predictions = y_predictions.flatten()\n",
    "y_train_predictions = y_train_predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "positivesSVMLinear = []\n",
    "negativesSVMLinear = []\n",
    "\n",
    "for i in range(len(y_predictions)):\n",
    "    if (y_test[i]) < -2:\n",
    "        positivesSVMLinear.append(-y_predictions[i]*100)\n",
    "    elif (y_test[i] <= 2):\n",
    "        negativesSVMLinear.append(-y_predictions[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "        \n",
    "svmLinearPlot = uOrfTools.rocPlot(fig, positivesSVMLinear, negativesSVMLinear, color = \"purple\", tmin = -100, tmax = 700)\n",
    "fig.savefig(\"rocSVMLinearTest.png\")\n",
    "save_ROC(\"ROC_svm_linear.csv\",positivesSVMLinear,negativesSVMLinear)\n",
    "#fig2 = plt.figure()\n",
    "#b = plt.boxplot((positives,negatives))\n",
    "\n",
    "#fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot for training data - SVM Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "positivesSVMLinear = []\n",
    "negativesSVMLinear = []\n",
    "\n",
    "for i in range(len(y_train_predictions)):\n",
    "    if (y_train[i]) < -2:\n",
    "        positivesSVMLinear.append(-y_train_predictions[i]*100)\n",
    "    elif (y_train[i] <= 2):\n",
    "        negativesSVMLinear.append(-y_train_predictions[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "        \n",
    "svmLinearPlot = uOrfTools.rocPlot(fig, positivesSVMLinear, negativesSVMLinear, color = \"purple\", tmin = -100, tmax = 700)\n",
    "fig.savefig(\"rocSVMLinearTrain.png\")\n",
    "save_ROC(\"ROC_svm_linear.train.csv\",positivesSVMLinear,negativesSVMLinear)\n",
    "#fig2 = plt.figure()\n",
    "#b = plt.boxplot((positives,negatives))\n",
    "\n",
    "#fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot for all genes - SVM Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "positivesSVMLinear = []\n",
    "negativesSVMLinear = []\n",
    "\n",
    "X = np.hstack((y_test,y_train))\n",
    "Y = np.hstack((y_predictions,y_train_predictions))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if (X[i]) < -2:\n",
    "        positivesSVMLinear.append(-Y[i]*100)\n",
    "    elif (X[i] <= 2):\n",
    "        negativesSVMLinear.append(-Y[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "        \n",
    "svmLinearPlot = uOrfTools.rocPlot(fig, positivesSVMLinear, negativesSVMLinear, color = \"purple\", tmin = -100, tmax = 700)\n",
    "fig.savefig(\"rocSVMLinearAll.png\")\n",
    "save_ROC(\"ROC_svm_linear.full.csv\",positivesSVMLinear,negativesSVMLinear)\n",
    "#fig2 = plt.figure()\n",
    "#b = plt.boxplot((positives,negatives))\n",
    "\n",
    "#fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest regressor grid search for best hyper parameters\n",
    "#Ignore warnings about ravel data like the ones below\n",
    "#Please change the shape of y to (n_samples,), for example using ravel().\n",
    "#because they take up too much space and distract my attention from the real values that I try to print\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Get all the features i.e. all the columns names\n",
    "finalCols = dataFrame.columns\n",
    "#Take all columns names of features except last column name which has label\n",
    "finalFeatures = finalCols[:-1]\n",
    "# Get values of all features\n",
    "x = dataFrame.loc[:, finalFeatures].values\n",
    "# Get all values of the label\n",
    "y = dataFrame.loc[:,['Translation Efficiency Value']].values\n",
    "\n",
    "print(finalFeatures)\n",
    "\n",
    "#Stanardize the data because the scales of the features have a large range\n",
    "scaler = StandardScaler().fit(x)\n",
    "x_std_data = scaler.transform(x)\n",
    "# y_std_data = y.round(1)\n",
    "y_std_data = y\n",
    "\n",
    "\n",
    "#####KEEP COMMENTED UNLESS YOU WANT TO RUN IT BECAUSE IT TAKES ABOUT 2 HOURS TO RUN\n",
    "#Split data into test and train\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_std_data, y_std_data, test_size = 0.33)  \n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# param_grid = [{'n_estimators':[10,30,60,80,100],'max_features':[2,4,6,8], \\\n",
    "#                'max_depth':[20,40,60,80,100], 'bootstrap':[True,False], \\\n",
    "#                'min_samples_split':[2,5,10], 'min_samples_leaf':[1,2,4]\\\n",
    "#               }]\n",
    "# forest_reg_GSCV = RandomForestRegressor(warm_start=True)\n",
    "# grid_search = GridSearchCV(forest_reg_GSCV, param_grid, \n",
    "#                            cv = 5, scoring = 'neg_mean_squared_error')\n",
    "# grid_search.fit(x_std_data,y_std_data)\n",
    "# print(grid_search.best_params_)\n",
    "# print(grid_search.best_estimator_)\n",
    "# cvres = grid_search.cv_results_\n",
    "# for mean_score, params in zip(cvres[\"mean_test_score\"],cvres[\"params\"]):\n",
    "#     print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch gives us the best estimator as\n",
    "# RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=60,\n",
    "#            max_features=2, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "#            min_impurity_split=None, min_samples_leaf=4,\n",
    "#            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
    "#            n_estimators=80, n_jobs=None, oob_score=False,\n",
    "#            random_state=None, verbose=0, warm_start=True)\n",
    "#Random Forest regressor\n",
    "#Ignore warnings about ravel data like the ones below\n",
    "#Please change the shape of y to (n_samples,), for example using ravel().\n",
    "#because they take up too much space and distract my attention from the real values that I try to print\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "import numpy as np\n",
    "\n",
    "#Get all the features i.e. all the columns names\n",
    "finalCols = dataFrame.columns\n",
    "#Take all columns names of features except last column name which has label\n",
    "finalFeatures = finalCols[:-1]\n",
    "# Get values of all features\n",
    "x = dataFrame.loc[:, finalFeatures].values\n",
    "# Get all values of the label\n",
    "y = dataFrame.loc[:,['Translation Efficiency Value']].values\n",
    "\n",
    "\n",
    "#Standardize the data because the scales of the features have a large range\n",
    "scaler = StandardScaler().fit(x)\n",
    "x_std_data = scaler.transform(x)\n",
    "y_std_data = y\n",
    "print(\"max TE:\", np.max(y))\n",
    "print(\"min TE:\", np.min(y))\n",
    "\n",
    "rmse_val = [] #to store rmse test values for different runs\n",
    "r2=[] #to store r2 test values for different runs\n",
    "ev=[]\n",
    "y_preds=[]\n",
    "rmse_train=[]\n",
    "r2_train=[]\n",
    "ev_train=[]\n",
    "y_train_preds=[]\n",
    "\n",
    "####Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(bootstrap=True,warm_start=True, min_samples_leaf=4,\n",
    "                                n_estimators=80,min_samples_split=10,max_features=2,max_depth=60)\n",
    "\n",
    "#########\n",
    "##DO 100 RUNS of Random forest regressor predictions then plot the variation in RMSE AND r2\n",
    "## Will give an idea of the variance in RMSE AND R2\n",
    "########\n",
    "for K in range(1,100):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_std_data, y_std_data, test_size = 0.2)\n",
    "    rf_model.fit(x_train, y_train.ravel())  #fit the model\n",
    "    y_predictions=rf_model.predict(x_test) #make prediction on test set\n",
    "    #for test data\n",
    "    error = np.sqrt(mean_squared_error(y_test,y_predictions)) #calculate rmse\n",
    "    r2.append(r2_score(y_test, y_predictions, multioutput = 'variance_weighted'))\n",
    "    ev.append(explained_variance_score(y_test, y_predictions))\n",
    "    rmse_val.append(error) \n",
    "    y_preds.append(y_predictions)\n",
    "    #for train data\n",
    "    y_train_predictions = rf_model.predict(x_train) #make prediction on train set\n",
    "    rmse_train.append(np.sqrt(mean_squared_error(y_train, y_train_predictions))) \n",
    "    r2_train.append(r2_score(y_train, y_train_predictions, multioutput = 'variance_weighted'))\n",
    "    ev_train.append(explained_variance_score(y_train, y_train_predictions))   \n",
    "    y_train_preds.append(y_train_predictions)\n",
    "    K = K+1\n",
    "\n",
    "\n",
    "#TODO later:check to what extent weights are most prominent\n",
    "#throw out the factors it is not making use on\n",
    "#do we do as well\n",
    "#Do prints to check the values\n",
    "print(\"The outputs\")\n",
    "print(\"Test mean r2:\", np.mean(r2))\n",
    "print(\"Test median r2:\", np.median(r2))\n",
    "print(\"Test mean rmse:\", np.mean(rmse_val))\n",
    "print(\"Test median rmse:\", np.median(rmse_val))\n",
    "print(\"Train mean r2:\", np.mean(r2_train))\n",
    "print(\"Train median r2:\", np.median(r2_train))\n",
    "print(\"Train mean rmse:\", np.mean(rmse_train))\n",
    "print(\"Train median rmse:\", np.median(rmse_train))\n",
    "\n",
    "#Create plots to see how RMSE and R2 vary between test and train data\n",
    "xAxis = []\n",
    "r2_yAxis =[]\n",
    "r2_train_yAxis =[]\n",
    "rmse_yAxis =[]\n",
    "rmse_train_yAxis =[]\n",
    "for i in range(1, 99):\n",
    "    #print(\"Test r2:\", r2[i], \"Train r2:\", r2_train[i], \"Test RMSE:\", rmse_val[i], \"Train RMSE:\", rmse_train[i])\n",
    "    xAxis.append(i)\n",
    "    r2_yAxis.append(r2[i])\n",
    "    r2_train_yAxis.append(r2_train[i])\n",
    "    rmse_yAxis.append(rmse_val[i])\n",
    "    rmse_train_yAxis.append(rmse_train[i])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Compare R2 of test and train\")\n",
    "plt.plot(xAxis,r2_yAxis, color = 'blue')\n",
    "plt.plot(xAxis,r2_train_yAxis, color = 'orange')\n",
    "fig = plt.figure()\n",
    "plt.title(\"Compare RMSE of test and train\")\n",
    "plt.plot(xAxis,rmse_yAxis, color = 'blue')\n",
    "plt.plot(xAxis,rmse_train_yAxis, color = 'orange')\n",
    "\n",
    "print(\"Minimum R2 Test:\", np.min(r2_yAxis))\n",
    "print(\"Minimum R2 Train:\", np.min(r2_train_yAxis))\n",
    "print(\"Max R2 Test:\", np.max(r2_yAxis))\n",
    "print(\"Max R2 Train:\", np.max(r2_train_yAxis))\n",
    "print(\"Minimum RMSE Test:\", np.min(rmse_yAxis))\n",
    "print(\"Minimum RMSE Train:\", np.min(rmse_train_yAxis))\n",
    "print(\"Max RMSE Test:\", np.max(rmse_yAxis))\n",
    "print(\"Max RMSE Train:\", np.max(rmse_train_yAxis))\n",
    "\n",
    "df = pd.DataFrame({'Actual':y_test.flatten(),'Predicted':y_predictions.flatten()})\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Put this in new cell###\n",
    "fig = plt.figure()\n",
    "plt.title(\"Compare predictions with actuals\")\n",
    "x_axis = []\n",
    "for i in range(0,len(y_predictions)):\n",
    "    x_axis.append(i)\n",
    "plt.plot(x_axis,y_test.flatten(), color = 'orange')\n",
    "plt.plot(x_axis,y_predictions.flatten(), color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "#ROC plot for Random Forest Regressor \n",
    "fig = plt.figure()\n",
    "positivesForest = []\n",
    "negativesForest = []\n",
    "\n",
    "for i in range(len(y_train_predictions)):\n",
    "    if (y_train[i]) < -2:\n",
    "        positivesForest.append(-y_train_predictions[i]*100)\n",
    "    elif (y_train[i] <= 2):\n",
    "        negativesForest.append(-y_train_predictions[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "        \n",
    "treePlot = uOrfTools.rocPlot(fig, positivesForest, negativesForest, color = \"purple\", tmin = -100, tmax = 700)\n",
    "save_ROC(\"ROC_rForest.train.csv\",positivesForest,negativesForest)\n",
    "fig.savefig(\"rocForestTrain.png\")\n",
    "#fig2 = plt.figure()\n",
    "#b = plt.boxplot((positives,negatives))\n",
    "\n",
    "#fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "#ROC plot for Random Forest Regressor \n",
    "fig = plt.figure()\n",
    "positivesForest = []\n",
    "negativesForest = []\n",
    "\n",
    "for i in range(len(y_predictions)):\n",
    "    if (y_test[i]) < -2:\n",
    "        positivesForest.append(-y_predictions[i]*100)\n",
    "    elif (y_test[i] <= 2):\n",
    "        negativesForest.append(-y_predictions[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "        \n",
    "treePlot = uOrfTools.rocPlot(fig, positivesForest, negativesForest, color = \"purple\", tmin = -100, tmax = 700)\n",
    "save_ROC(\"ROC_Forest.csv\",positivesForest,negativesForest)\n",
    "fig.savefig(\"rocForestTest.png\")\n",
    "#fig2 = plt.figure()\n",
    "#b = plt.boxplot((positives,negatives))\n",
    "\n",
    "#fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "y_predictions = y_predictions.flatten()\n",
    "y_train_predictions = y_train_predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest roc plot\n",
    "fig = plt.figure()\n",
    "positivesForest = []\n",
    "negativesForest = []\n",
    "X = np.hstack((y_train,y_test))\n",
    "Y = np.hstack((y_train_predictions,y_predictions))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if (X[i] < -2):\n",
    "        #Question for Dr. Voorhies: why should I take -ve of y_predictions\n",
    "        positivesForest.append(-Y[i]*100)\n",
    "    elif (X[i] <=2):\n",
    "        negativesForest.append(-Y[i]*100)\n",
    "        \n",
    "#print(len(set(positives)), len(set(negatives)))\n",
    "#print len(positives)\n",
    "#print len(negatives)\n",
    "        \n",
    "forestPlot = uOrfTools.rocPlot(fig, positivesForest, negativesForest, color = \"green\", tmin = -100, tmax = 700)\n",
    "uOrfTools.rocPlot(fig, positiveOrf, negativeOrf, color = \"red\", tmin = -100, tmax = 700)\n",
    "fig.savefig(\"rocForestAll.png\")\n",
    "save_ROC(\"ROC_forest.full.csv\",positives,negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do cross fold validation for different algorithms\n",
    "#Cross validation\n",
    "\n",
    "def display_scores(rmse_score):\n",
    "    print(\"score:\", rmse_score)\n",
    "    print(\"mean:\", rmse_score.mean())\n",
    "    print(\"standard deviation:\", rmse_score.std())\n",
    "\n",
    "print np.max(y_std_data)\n",
    "print np.min(y_std_data)\n",
    "\n",
    "#cv random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "forest_reg = RandomForestRegressor()\n",
    "scores = cross_val_score(forest_reg, x_std_data, \\\n",
    "                         y_std_data, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "rmse_score = np.sqrt(-scores)    \n",
    "print \"Random Forest Regressor CV results\" \n",
    "display_scores(rmse_score)\n",
    "print forest_reg\n",
    "\n",
    "#cv decision tree\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "scores = cross_val_score(tree_reg, x_std_data, \n",
    "                         y_std_data, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "rmse_score = np.sqrt(-scores)    \n",
    "print \"Decision Tree Regressor CV results\" \n",
    "display_scores(rmse_score)\n",
    "print tree_reg\n",
    "\n",
    "#cv linear regression\n",
    "linear_reg = LinearRegression()\n",
    "scores = cross_val_score(linear_reg, x_std_data, \\\n",
    "                         y_std_data, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "rmse_score = np.sqrt(-scores)    \n",
    "print \"Linear Regressor CV results\" \n",
    "display_scores(rmse_score)\n",
    "print(linear_reg)\n",
    "print linear_reg\n",
    "\n",
    "#cv ridge\n",
    "#lasso_reg = linear_model.Lasso(alpha = 0.00042887)\n",
    "ridge_reg = linear_model.Ridge(alpha = 0.01, normalize = True)\n",
    "scores = cross_val_score(ridge_reg, x_std_data, \\\n",
    "                         y_std_data, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "rmse_score = np.sqrt(-scores)    \n",
    "print \"Ridge Regressor CV results\" \n",
    "display_scores(rmse_score)\n",
    "print ridge_reg\n",
    "\n",
    "#cv lasso\n",
    "lasso_reg = linear_model.Lasso(alpha = 0.00042887)\n",
    "scores = cross_val_score(lasso_reg, x_std_data, \\\n",
    "                         y_std_data, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "rmse_score = np.sqrt(-scores)    \n",
    "print \"Lasso Regressor CV results\" \n",
    "display_scores(rmse_score)\n",
    "print lasso_reg\n",
    "\n",
    "#cv svm linear\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_linear_reg = SVR(kernel = \"linear\", C = 10.0, epsilon = 0.2)\n",
    "scores = cross_val_score(svm_linear_reg, x_std_data, \\\n",
    "                         y_std_data.ravel(), scoring = 'neg_mean_squared_error', cv = 5)\n",
    "rmse_score = np.sqrt(-scores)    \n",
    "print \"SVM Linear Regressor CV results\" \n",
    "display_scores(rmse_score)\n",
    "print svm_linear_reg\n",
    "\n",
    "#cv svm poly\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel = \"poly\", degree = 2, C = 10.0, epsilon = 0.2)\n",
    "scores = cross_val_score(svm_poly_reg, x_std_data, \\\n",
    "                         y_std_data.ravel(), scoring = 'neg_mean_squared_error', cv = 5)\n",
    "rmse_score = np.sqrt(-scores)    \n",
    "print \"SVM Poly Regressor CV results\" \n",
    "display_scores(rmse_score)\n",
    "print svm_poly_reg\n",
    "\n",
    "#cv svm sigmoid\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_sigmoid_reg = SVR(kernel = \"sigmoid\", C = 1.0, epsilon = 0.2)\n",
    "scores = cross_val_score(svm_sigmoid_reg, x_std_data, \\\n",
    "                         y_std_data.ravel(), scoring = 'neg_mean_squared_error', cv = 5)\n",
    "rmse_score = np.sqrt(-scores)    \n",
    "print \"SVM Sigmoid Regressor CV results\" \n",
    "display_scores(rmse_score)\n",
    "print svm_sigmoid_reg\n",
    "\n",
    "#cv svm default\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_reg = SVR(C = 10.0, epsilon = 0.2)\n",
    "scores = cross_val_score(svm_reg, x_std_data, \\\n",
    "                         y_std_data.ravel(), scoring = 'neg_mean_squared_error', cv = 5)\n",
    "rmse_score = np.sqrt(-scores)    \n",
    "print \"SVM Default Regressor CV results\" \n",
    "display_scores(rmse_score)\n",
    "print svm_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utrList = load_ROC(\"ROC_utr.csv\")\n",
    "\n",
    "positiveutr = utrList[0]\n",
    "negativeutr = utrList[1]\n",
    "\n",
    "orfList = load_ROC(\"uOrfTEATG.csv\")\n",
    "\n",
    "positiveOrf = orfList[0]\n",
    "negativeOrf = orfList[1]\n",
    "\n",
    "orfNoATGList = load_ROC(\"uOrfTENoATG.csv\")\n",
    "\n",
    "positiveOrfNoATG = orfNoATGList[0]\n",
    "negativeOrfNoATG = orfNoATGList[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "uOrfTools.rocPlot(fig, positivesLasso, negativesLasso, color = \"black\", tmin = -100, tmax = 700)\n",
    "uOrfTools.rocPlot(fig, positivesRidge, negativesRidge, color = \"#afeeee\", tmin = -100, tmax = 700)\n",
    "uOrfTools.rocPlot(fig, positivesSVMLinear, negativesSVMLinear, color = \"teal\", tmin = -100, tmax = 700)\n",
    "uOrfTools.rocPlot(fig, positives, negatives, color = \"orange\", tmin = -100, tmax = 700) #linear regression\n",
    "uOrfTools.rocPlot(fig, positivesTree, negativesTree, color = \"green\", tmin = -100, tmax = 700)\n",
    "uOrfTools.rocPlot(fig, positiveutr, negativeutr, color = \"blue\", tmin = -100, tmax = 700)\n",
    "uOrfTools.rocPlot(fig, positivesForest, negativesForest, color = \"purple\", tmin = -100, tmax = 700)\n",
    "uOrfTools.rocPlot(fig, positiveOrf, negativeOrf, color = \"red\", tmin = -100, tmax = 700)\n",
    "uOrfTools.rocPlot(fig, positiveOrfNoATG, negativeOrfNoATG, color = \"pink\", tmin = -100, tmax = 700)\n",
    "fig.savefig(\"presAllPlot.png\")\n",
    "#fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "uOrfTools.rocPlot(fig, positiveOrf, negativeOrf, color = \"red\", tmin = -100, tmax = 700)\n",
    "uOrfTools.rocPlot(fig, positiveOrfNoATG, negativeOrfNoATG, color = \"pink\", tmin = -100, tmax = 700)\n",
    "fig\n",
    "fig.savefig(\"orfATGComparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "uOrfTools.rocPlot(fig, positiveutr, negativeutr, color = \"blue\", tmin = -100, tmax = 700)\n",
    "uOrfTools.rocPlot(fig, positiveOrf, negativeOrf, color = \"red\", tmin = -100, tmax = 700)\n",
    "fig.savefig(\"utrOrfComparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
